---
layout: single
title: "[ESL 리뷰] 교재 4-2장 Logistic Regression, Seperating Hyperplanes"
categories: ML
tag: [OLS, Logistic Regression, Seperating Hyperplances, SVM, Perceptron Learning]
typora-root-url: ..\
author_profile: false
use_math: true
toc: true
header:
  overlay_image: /images/2024-02-28-chaphter4-1/스크린샷 2024-01-31 213848.png
  overlay_filter: 0.5
---




$$
\log{\frac{Pr(G=k|X=x)}{Pr(G=K|X=x)}} = \beta_{k0} + \beta_k^Tx \ \text{for} \ k = 1,...,K-1
$$

$$
\theta = \{ \beta_{10}, \beta_1^T,...,\beta_{(K-1)0}, \beta_{K-1}^T, \ Pr(G=k|X=x) = p_k(x;\theta)
$$

$$
Pr(G=k|X=x) = \frac{\exp(\beta_{k0}+\beta_k^Tx)}{1+\sum_{l=1}^{K-1}\exp(\beta_{l0}+\beta_l^Tx)} \ \text{for} \ k = 1,...,K-1
$$

$$
Pr(G=K|X=x) = \frac{1}{1+\sum_{l=1}^{K-1}\exp(\beta_{l0}+\beta_l^Tx)}
$$

$$
\begin{align*}
l(\beta) &= \sum_{i=1}^N \{ y_i \log{p(x_i;\beta)} + (1-y_i) \log{(1-p(x_i;\beta))}\} \\
		 &= \sum_{i=1}^N \{ y_i\beta^Tx_i - \log{(1+\exp(\beta^Tx_i))} \}
\end{align*}
$$

$$
\frac{\partial l(\beta)}{\partial \beta} = \sum_{i=1}^N x_i(y_i - p(x_i;\beta)) = \mathbf{X}^T(\mathbf{y} - \mathbf{p}) \ \text{where} \ \mathbf{p} \ \text{the vector of} \ p(x_i;\beta^{old})
$$

$$
\frac{\partial^2 l(\beta)}{\partial \beta \partial \beta^T} = -\sum_{i=1}^Nx_ix_i^Tp(x_i;\beta)(1-p(x_i;\beta)) = -\mathbf{X}^T\mathbf{W}\mathbf{X} \ \text{where} \  \mathbf{W} \text{ a diagonal matrix with} \\ \mathbf{W}_{ii} = p(x_i;\beta^{old})(1-p(x_i;\beta^{old})).
$$

$$
\begin{align*}
\beta^{new} &= \beta^{old} + (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T(\mathbf{y}-\mathbf{p}) \\
			&= (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}(\mathbf{X}\beta^{old} +\mathbf{W}^{-1}(\mathbf{y} -\mathbf{p})) \\
			&= (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}\mathbf{z}, \ \mathbf{z}:\text{adjusted response}
\end{align*}
$$

$$
\beta^{new} \leftarrow \underset{\beta}{\mathrm{argmin}} (\mathbf{z}-\mathbf{X}\beta)^T\mathbf{W}(\mathbf{z}-\mathbf{X}\beta)
$$

$$
l(\beta) = \sum_{i=1}^N \sum_{k=1}^{K-1}[I(y_i = k)\beta_k^Tx_i - \log(1+\sum_{l=1}^{K-1}\exp(\beta_l^Tx_i))]
$$

$$
\mathbf{z} = \mathbf{X}\hat{\beta} +\mathbf{W}^{-1}(\mathbf{y} -\hat{\mathbf{p}})
$$

$$
\hat{\beta} = (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}\mathbf{z}
$$

$$
z_i = x_i^T\hat{\beta} + \frac{(y_i - \hat{p}_i)}{\hat{p}_i(1-\hat{p}_i)}, \ w_i = \hat{p_i}(1-\hat{p}_i)
$$

$$
D = -2l(\beta) + 2l(\text{perfect fitting})
$$

$$
\begin{align*}
D &= -2\sum_{i=1}^N \{ y_i \log{\hat{p}_i} + (1-y_i) \log{(1-\hat{p}_i)}\} + 2\sum_{i=1}^N \{ y_i \log{y_i} + (1-y_i) \log{(1-y_i)}\} \\
		 &= 2\sum_{i=1}^N [y_i \log{\frac{y_i}{\hat{p}_i}} + (1-y_i) \log{\frac{1-y_i}{1-\hat{p}_i}}]\\
		 &\approx 2\sum_{i=1}^N[ (y_i-\hat{p}_i) + \frac{(y_i-\hat{p}_i)^2}{2\hat{p}_i} + \{(1-y_i)-(1-\hat{p}_i)\} + \frac{\{(1-y_i)-(1-\hat{p}_i)\}^2}{2(1-\hat{p}_i)}] \\
		 &= \sum_{i=1}^N\frac{(y_i-\hat{p}_i)^2}{\hat{p_i}} + \frac{y_i-\hat{p}_i}{1-\hat{p}_i} \\
		 &= \sum_{i=1}^N\frac{(y_i-\hat{p}_i)^2}{\hat{p_i}(1-\hat{p_i})}
\end{align*}
$$

$$
\begin{align*}
\mathbb{E}[\hat{\beta}] &= \mathbb{E}[(\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}\mathbf{z}] \\
						&= (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}\mathbb{E}[\mathbf{X}\beta +\mathbf{W}^{-1}(\mathbf{y} -\mathbf{p})] \\
						&= (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}\mathbf{X}\beta =\beta
\end{align*}
$$

$$
y_i \overset{\mathrm{iid}}{\sim} Bernoulli(p_i)
$$

$$
\mathbb{E}[\mathbf{y}] = \mathbf{p}, \ Var[\mathbf{y}] = \mathbf{W}
$$

$$
\begin{align*}
Var[\hat{\beta}] &= Var[(\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}\mathbf{z}] \\
						&= (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}Var[\mathbf{X}\beta +\mathbf{W}^{-1}(\mathbf{y} -\mathbf{p})]((\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W})^T \\
						&= (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}(\mathbf{W}^{-1}\mathbf{W}{\mathbf{W}^{-1}}^T)((\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W})^T \\
						&= (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}
\end{align*}
$$

$$
\hat{\beta} \sim \mathcal{N}(\beta, (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1})
$$

$$
\underset{\beta_0,\beta}{\mathrm{max}} \left[ \sum_{i=1}^N\{y_i(\beta_0 + \beta^Tx_i) - \log(1+\exp(\beta_0 + \beta^Tx))\} - \lambda\sum_{j=1}^p|\beta_p| 	\right]
$$

$$
\frac{\partial l}{\partial \beta_j} = \mathbf{x}_j^T(\mathbf{y} - \mathbf{p}) - \lambda \cdot \text{sign}(\beta_j) = 0, \ \mathbf{x}_j^T(\mathbf{y} - \mathbf{p}) = \lambda \cdot \text{sign}(\beta_j)
$$

$$
\underset{\beta_0,\beta}{\mathrm{min}} \ D(\beta, \beta_0) = \underset{\beta_0,\beta}{\mathrm{min}}-\sum_{i \in \mathcal{M}} y_i(x_i^T\beta + \beta_0)
$$

$$
\frac{1}{\|\beta\|}y_i(x_i^T\beta + \beta) \geq M, \ \text{and set} \ \|\beta\| = \frac{1}{M}
$$

$$
\beta = \sum_{i=1}^N\alpha_iy_ix_i, \ \sum_{i=1}^N\alpha_iy_i = 0
$$

$$
L_D = \sum_{i=1}^N \alpha_i - \frac{1}{2}\sum_{i=1}^N\sum_{k=1}^N\alpha_i\alpha_ky_iy_kx_i^Tx_k  \ \text{subject to} \ \alpha_i \geq 0
$$

$$
1 - y_i(x_i^T\beta + \beta) \leq 0 \ \text{for all} \ i
$$

$$
\alpha_i [y_i(x_i^T\beta + \beta_0)-1] = 0 \ \text{for all} \ i.
$$

$$
\beta = \sum_{i\in\mathcal{S}}\alpha_iy_ix_i
$$



