---
layout: single
title: "[ESL 리뷰] 교재 3장-3 Forward Stagewise Selection, Least Angle Regression"
categories: ML
tag: [OLS,ESL, Forward Stagewise Selection, Least Angle Regression, Lasso,Shrinkage method, Subset selection]
typora-root-url: ..\
author_profile: false
use_math: true
toc: true
header:
  overlay_image: /images/2024-01-16-chapter3-2/ridge_and_lasso.png
  overlay_filter: 0.5
---



“**The Elements of Statistical Learning**” 교재 3장의 Shrinkage Methods중 Forward Stagewise Selection, Least Angle Regression(LAR)에 관해 정리를 해보았다.



# Forward Stagewise Selection

[Foward Stepwise Selection을 다룬 이전 포스팅](https://oymin2001.github.io/ml/chapter3-1/#forward-stepwise-selection)에서는 매 iteration마다 잔차 $\mathbf{r}$을 업데이트하면서 $\mathbf{x}_j^T\mathbf{r}$가 가장 큰  $j$를 찾아서(잔차와 상관관계가 가장 높은 변수) QR분해를 이용하여 해당변수를 추가해줬었다. Forward stagewise selection은 이와 비슷하지만 변수를 한번에 추가해주는것이 아닌 점진적으로 추가하는 방법이다. 알고리즘은 아래와 같다.

![FS_alg](/images/2024-01-17-chapter3-3/FS_alg.png)

3번 과정에서 $\epsilon$ 에 $\left\langle \mathbf{x}_j, \mathbf{r} \right\rangle$을 대입하면 Forward stepwise selection이다. 이후에 다룰 예정이지만 Forward stagewise selection은 라쏘와 상당히 달라보이지만, 어떠한 조건하($\epsilon$이 0에 매우 가까울 때)에서는 완전히는 아니지만 상당히 유사한 결과를 가진다.



&nbsp;





# Least Angle Regression

Least Angle Regression(LARS)은 Forward Stagewise selection을 굉장히 효율적?으로 개선된 모델이다. LARS의 해를 찾는데는 p번의 iteration만으로 충분하다. 만약 $p > N-1$일 경우 $N-1$번 이후로는 잔차가 0이 됨으로 업데이트가 이루어지지 않으므로 중단한다.(1을 뺀 이유는 데이터를 표준화했기 때문) 



LARS의 간단하게 예시를 들자면 다음과 같이 작동한다. 먼저, 이전 Forward stagewise selection과 같이 모든 회귀 계수를 0으로 초기화하고 잔차와 가장 상관관계가 높은 $\mathbf{x}_1$를 찾는다. 이후에는 다른 잔차와 상관관계가 높은 $\mathbf{x}_2$가 나오기 전까지 가능한 한 크게 $\mathbf{x}_1$방향으로 움직인다. 여기까지는 Forward stagewise selection와 같다. 이후 업데이트 부터가 차이가 있는데, 다음 업데이트에서 $\mathbf{x}_2$의 방향으로 업데이트를 하는것이 아닌, $\mathbf{x}_1$,$\mathbf{x}_2$ 두 벡터와 같은 각을 이루는 새로운 벡터 $\mathbf{u}_2$의 방향으로 업데이트를 진행한다. 



p=2인 경우의 LARS는 다음과 같이 진행된다. $\hat{\mathbf{\mu}}_0 = \mathbf{X}\beta = 0$를 잡는다. ($\mathbf{X} = (\mathbf{x}_1, \mathbf{x_2}), \ \beta_1=\beta_2=0$) 아래 그림과 같이 $\mathbf{y}$를 $\mathbf{X}$의 열공간에 정사영시킨 벡터를 $\bar{\mathbf{y}}_2$라 하면,  잔차 $\mathbf{c}(\hat{\mathbf{\mu}})$는 다음과 같이 구할 수 있다. 


$$
\mathbf{c}(\hat{\mathbf{\mu}}) = \mathbf{X}^T(\mathbf{y} - \hat{\mathbf{\mu}}) = \mathbf{X}^T(\bar{\mathbf{y}}_2 - \hat{\mathbf{\mu}})
$$


아래 그림에서 $\bar{\mathbf{y}}_2 - \hat{\mathbf{\mu}}\$와 더 작은 각을 이루는(상관관계가 더 높은) 벡터는 $\mathbf{x}_1$이므로 (다시 말해서, $\mathbf{c}_1(\hat{\mathbf{\mu}}_0) > \mathbf{c}_2(\hat{\mathbf{\mu}}_0)$),  $\mathbf{x}_1$방향으로 업데이트를 진행한다. 즉, $\hat{\mathbf{\mu}}_1 = \hat{\mathbf{\mu}}_0 + \hat{\gamma}_1 \mathbf{x}_1$이다. FS의 경우에서는 $\hat{\gamma}_1$에 $\epsilon$값을 사용하였지만, LARS는 $\bar{\mathbf{y}}_2 - \hat{\mathbf{\mu}}_1$이 $\mathbf{x}_1, \ \mathbf{x}_2$와 같은 각을 이루도록  $\hat{\gamma}_1$을 잡는다. 즉, $\mathbf{c}_1(\hat{\mathbf{\mu}}_1) = \mathbf{c}_2(\hat{\mathbf{\mu}}_1)$인 $\hat{\gamma}_1$를 찾는다.





![LAR_2dim](/images/2024-01-17-chapter3-3/LAR_2dim.png)

위 그림과 같이 $\mathbf{u}_2$를  $\mathbf{x}_1, \ \mathbf{x}_2$와 같은 각을 이루는 단위벡터라고 하자. LARS의 다음 스텝은 $\hat{\mathbf{\mu}}_2 = \hat{\mathbf{\mu}}_1 + \hat{\gamma}_2 \mathbf{x}_2$로, $\hat{\mathbf{\mu}}_2 = \bar{\mathbf{y}}_2$이 되도록 $\hat{\gamma}_2$을 잡는다. 만약 $p$가 2보다 클 경우에는 다음 $\mathbf{x}_3$와 같은 각을 이루도록, $\hat{\gamma}_2$를 더 작게 잡아 방향을 바꾼다.

 ![LAR_3dim](/images/2024-01-17-chapter3-3/LAR_3dim.png)



LARS의 작동과정은 알아봤으니 이제 $\hat{\gamma}_1,\hat{\gamma}_2,... $들을 구하는 방법을 알아보자. 먼저 $\mathbf{X}$의 열벡터는 모두 선형독립이라고 가정하자. $\mathcal{A}$를 현재 선택된 변수들의 인덱스를 모아놓은 집합이라고 하고,  다음과 같이 정의해보자.


$$
\begin{align*}
\mathbf{X}_{\mathcal{A}} &= (..., s_j\mathbf{x}_j,...)_{j\in \mathcal{A}}, \ s_j = sign(\left\langle \mathbf{x}_j, \mathbf{r} \right\rangle) \\
\mathbf{\mathcal{G}}_{\mathcal{A}} &= \mathbf{X}_{\mathcal{A}}^T\mathbf{X}_{\mathcal{A}} \\
\mathbf{A}_{\mathcal{A}} &= (1_{\mathcal{A}}^T \mathbf{\mathcal{G}}^{-1} 1_{\mathcal{A}})^{-\frac{1}{2}} \in \mathbb{R} \ \text{where} \ \ 1_{\mathcal{A}} \in \mathbb{R}^{|\mathcal{A}|} \text{is  a vector of 1's}

\end{align*}
$$
