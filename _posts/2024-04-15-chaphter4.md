---
layout: single
title: "[ESL 리뷰] 교재 4장"
categories: ML
tag: [OLS,Logistic Regression]
typora-root-url: ..\
author_profile: false
use_math: true
toc: true

---


$$
\hat{\mathbf{Y}} = \mathbf{X} (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} = \mathbf{X}\hat{\mathbf{B}}
$$

$$
\hat{f}(x)^T = (1, x^T) \hat{\mathbf{B}} = (\hat{f}_1(x), ... ,\hat{f}_K(x)) \in \mathbb{R}^{1 \times K}
$$



$$
\hat{G}(x) = \underset{k\in\mathcal{G}}{\mathrm{argmax}} \hat{f}_k(x)
$$

$$
Y_k, \ \mathbb{E}(Y_k|X=x) =  Pr(G=K|X=x)
$$

$$
\mathbf{X} = \begin{pmatrix}
\mathbf{1}_N \ \mathbf{x} 
\end{pmatrix} \ \text{where} \ \mathbf{x} = (x_1,...,x_N)^T
$$

$$
\mathbf{X}^T \mathbf{X} = \begin{pmatrix}
\mathbf{1}_N^T \\ \mathbf{x}^T 
\end{pmatrix}\begin{pmatrix}
\mathbf{1}_N & \mathbf{x}
\end{pmatrix} = \begin{pmatrix}
N & \mathbf{1}_N^T\mathbf{x} \\ \mathbf{x}^T\mathbf{1}_N & \mathbf{x}^T\mathbf{x}  
\end{pmatrix}
$$

$$
\begin{align*}
\sum_{k \in \mathcal{G}} \hat{f}_k(x) &= (1, x^T) \hat{\mathbf{B}}\mathbf{1}_K \\
									  &= (1, x^T)(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}\mathbf{1}_K \\
									  &= (1, x^T)\begin{pmatrix}
N & \mathbf{1}_N^T\mathbf{x} \\ \mathbf{x}^T\mathbf{1}_N & \mathbf{x}^T\mathbf{x}  
\end{pmatrix}^{-1}\begin{pmatrix}
\mathbf{1}_N^T \\ \mathbf{x}^T 
\end{pmatrix}\mathbf{1}_N \\
									 &= (1, x^T)\begin{pmatrix}
N & \mathbf{1}_N^T\mathbf{x} \\ \mathbf{x}^T\mathbf{1}_N & \mathbf{x}^T\mathbf{x}  
\end{pmatrix}^{-1}  \begin{pmatrix}
N \\ \mathbf{x}^T\mathbf{1}_N 
\end{pmatrix} = (1,x^T)\begin{pmatrix}
1 \\ \mathbf{0}_p 
\end{pmatrix} = 1

\end{align*}
$$

$$
Pr(X=x) =\sum_{k\in \mathcal{G}}Pr(X=x|G=k)Pr(G=k)
$$

$$
Pr(G=k|X=x) = \frac{Pr(X=x|G=k)Pr(G=k)}{Pr(X=x)} = \frac{f_k(x)\pi_k}{\sum_{l=1}^K f_l(x)\pi_l}
$$

$$
f_k(x) = \mathcal{N}(\mu_k, \Sigma_k) = \frac{1}{(2\pi)^{p/2}|\Sigma_k|^{1/2}}\exp(-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k))
$$

$$
\{ x: Pr(G=k|X=x) = Pr(G=l | X=x) \} = \{ x: \log{\frac{Pr(G=k|X=x)}{Pr(G=l | X=x)}=0} \}
$$

$$
\delta_k(x) = x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log \pi_k 
$$

$$
\begin{align*}
\hat{\pi}_k &= \frac{N_k}{N} \ \text{where} \ N_k  \ \text{is the number of class-k observations} \\
\hat{\mu}_k &= \frac{1}{N_k}\sum_{g_i = k}x_i \\
\hat{\Sigma}&= \sum_{k=1}^K\sum_{g_i = k}\frac{1}{N-k}(x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T \ \text{(pooled covariance, within-class covariance)}
\end{align*}
$$




$$
\bar{\mu} = \frac{1}{N}\sum_{i=1}^N x_i  = \frac{1}{N}\sum_{k=1}^KN_k\hat{\mu}_k
$$

$$
\begin{align*}
\hat{\Sigma}_T &= \frac{1}{N}\sum_{i=1}^N (x_i-\bar{\mu})(x_i-\bar{\mu})^T = \frac{1}{N} \left[ \sum_{i=1}^N x_ix_i^T - N\bar{\mu}\bar{\mu}^T \right]  \ \text{(total covariance)} \\
\hat{\Sigma}_W &= \frac{1}{N}\sum_{k=1}^K\sum_{g_i = k}(x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T =  \frac{1}{N}\sum_{k=1}^K\left[ \sum_{g_i = k} x_ix_i^T - N_k\hat{\mu}_k\hat{\mu}_k^T \right] = \frac{1}{N}\left[ \sum_{i=1}^N x_ix_i^T - \sum_{k=1}^KN_k\hat{\mu}_k\hat{\mu}_k^T \right]  \ \text{(within-class covariance)} \\
\hat{\Sigma}_B &= \frac{1}{N}\sum_{k=1}^KN_k(\hat{\mu}_k - \bar{\mu})(\hat{\mu}_k - \bar{\mu})^T = \frac{1}{N}\left[ \sum_{k=1}^KN_k\hat{\mu}_k\hat{\mu}_k^T - N\bar{\mu}\bar{\mu}^T \right]  \ \text{(between-class covariance)}

\end{align*}
$$



$$
\begin{align*}
\hat{\Sigma}_W &= \frac{1}{N}\sum_{k=1}^K\sum_{g_i = k}(x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T \\
               &=  \frac{1}{N}\sum_{k=1}^K\left[ \sum_{g_i = k} x_ix_i^T - N_k\hat{\mu}_k\hat{\mu}_k^T \right] = \frac{1}{N}\left[ \sum_{i=1}^N x_ix_i^T - \sum_{k=1}^KN_k\hat{\mu}_k\hat{\mu}_k^T \right]  \ \text{(within-class covariance)}
\end{align*}
$$








$$
x^T\Sigma^{-1}(\hat{\mu}_2 - \hat{\mu}_1) >  \frac{1}{2}\hat{\mu}_2^T\Sigma^{-1}\hat{\mu}_2 - \frac{1}{2}\hat{\mu}_1^T\Sigma^{-1}\hat{\mu}_1 + \log \frac{N_1}{N} - \log \frac{N_2}{N}
$$

$$
\begin{align*}
\mathbf{X}^T\mathbf{X}\begin{pmatrix}
\hat{\beta}_0 \\ \hat{\beta} 
\end{pmatrix} &= \mathbf{X}^T\mathbf{Y} \\
\begin{pmatrix}
N & \mathbf{1}_N^T\mathbf{x} \\ \mathbf{x}^T\mathbf{1}_N & \mathbf{x}^T\mathbf{x}  
\end{pmatrix}\begin{pmatrix}
\hat{\beta}_0 \\ \hat{\beta} 
\end{pmatrix} &=\begin{pmatrix} \mathbf{1}_N^T\mathbf{Y} \\ \mathbf{x}^T\mathbf{Y}\end{pmatrix}\\


\end{align*}
$$

$$
\begin{align*}
\hat{\beta}_0 &= \frac{1}{N}\sum_{i=1}^NY_i - \frac{1}{N}\sum_{i=1}^Nx_i^T\hat{\beta} = - \frac{1}{N}\sum_{i=1}^Nx_i^T\hat{\beta} \\
(\sum_{i=1}^N x_ix_i^T - \frac{1}{N}\sum_{i=1}^N x_i\sum_{i=1}^N x_i^T)\hat{\beta} &= \sum_{i=1}^NY_ix_i - \frac{1}{N} \sum_{i=1}^N Y_i\sum_{i=1}^N x_i = \sum_{i=1}^NY_ix_i
\end{align*}
$$

$$
\sum_{i=1}^N x_i =\sum_{i:g_i=1}x_i + \sum_{i:g_i=2}x_i =  N_1\hat{\mu}_1 + N_2\hat{\mu}_2, \ \sum_{i=1}^N Y_i = 0, \ \sum_{i=1}^Nx_iY_i = -\frac{N}{N_1}N_1\hat{\mu}_1 + \frac{N}{N_2}N_2\hat{\mu}_2 = N(\hat{\mu}_2 - \hat{\mu_1})
$$

$$
\begin{align*}
\hat{\Sigma} &= \frac{1}{N-2}(\sum_{i:g_i=1}(x_i-\hat{\mu}_1)(x_i-\hat{\mu}_1)^T + \sum_{i:g_i=2}(x_i-\hat{\mu}_2)(x_i-\hat{\mu}_2)^T) \\
			 &= \frac{1}{N-2}(\sum_{i=1}^Nx_ix_i^T -N_1\hat{\mu}_1\hat{\mu}_1^T - N_2\hat{\mu}_2\hat{\mu}_2^T)
\end{align*}
$$

$$
\begin{align*}
(\sum_{i=1}^N x_ix_i^T - \frac{1}{N}\sum_{i=1}^N x_i\sum_{i=1}^N x_i^T)\hat{\beta} &= ((N-2)\hat{\Sigma} + N_1\hat{\mu}_1\hat{\mu}_1^T + N_2\hat{\mu}_2\hat{\mu}_2^T - \frac{1}{N}(N_1\hat{\mu}_1+N_2\hat{\mu}_2)(N_1\hat{\mu}_1^T+N_2\hat{\mu}_2^T))\hat{\beta} \\
&=  ((N-2)\hat{\Sigma} +\frac{N_1N_2}{N}(\hat{\mu}_2-\hat{\mu}_1)(\hat{\mu}_2-\hat{\mu}_1)^T)\hat{\beta} = N(\hat{\mu}_2 - \hat{\mu_1})
\end{align*}
$$

$$
\begin{align*}
\hat{\Sigma}_{B} &= \frac{1}{N}\{N_1(\hat{\mu}_1 - \frac{\hat{\mu}_1+\hat{\mu}_2}{2})(\hat{\mu}_1 - \frac{\hat{\mu}_1+\hat{\mu}_2}{2})^T + N_2(\hat{\mu}_2 - \frac{\hat{\mu}_1+\hat{\mu}_2}{2})(\hat{\mu}_2 - \frac{\hat{\mu}_1+\hat{\mu}_2}{2})^T\}\\  &= \frac{1}{4}(\hat{\mu}_2 - \hat{\mu}_1)(\hat{\mu}_2 - \hat{\mu}_1)^T
\end{align*}
$$

$$
\hat{\beta} \propto \hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1)
$$

$$
\hat{\beta}_0 =- \frac{1}{N}\sum_{i=1}^Nx_i^T\hat{\beta}= -\frac{1}{N}( N_1\hat{\mu}_1^T + N_2\hat{\mu}_2^T)\hat{\beta}
$$

$$
\begin{align*}
\hat{f}(x) &= \hat{\beta}_0 + x^T\hat{\beta} = [x^T - \frac{1}{N}(N_1\hat{\mu}_1^T+N_2\hat{\mu}_2^T)]\hat{\beta} \\ &\propto [x^T - \frac{1}{N}(N_1\hat{\mu}_1^T+N_2\hat{\mu}_2^T)]\hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1) > 0
\end{align*}
$$

$$
x^T\hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1) > 
\frac{N_2}{N}\hat{\mu_2}^T\hat{\Sigma}^{-1}\hat{\mu}_2 - \frac{N_1}{N}\hat{\mu_1}^T\hat{\Sigma}^{-1}\hat{\mu}_1 + \frac{N_2-N_1}{N}\hat{\mu}_1^T\hat{\Sigma}^{-1}\hat{\mu}_2
$$

$$
X^* \leftarrow \mathbf{D}^{-\frac{1}{2}}\mathbf{U}^TX, \ \text{where} \ \hat{\Sigma} = \mathbf{U}\mathbf{D}\mathbf{U}^T
$$

$$
\delta_k(x^*) = \log[\exp(-\frac{1}{2}(x^*-\mu_k^*)^T(x^*-\mu_k^*))\pi_k] = -\frac{1}{2}\|x^*-\mu_k^*\|^2 + \pi_k
$$

$$
\hat{G}(x) = \underset{k\in\mathcal{G}}{\mathrm{argmax}} f_k(x)\pi_k = \underset{k\in\mathcal{G}}{\mathrm{argmax}} -\frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k)  + \log{\pi_k}
$$

$$
\begin{align*}
\hat{G}(x) &= \underset{k\in\mathcal{G}}{\mathrm{argmax}} \left( x^T (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}\right)_k + \frac{N_k}{N} \\
		   &= \underset{k\in\mathcal{G}}{\mathrm{argmax}} \ x^T\hat{\Sigma}_T^{-1}\frac{1}{N}\sum_{g_i = k}x_i + \frac{N_k}{N} \\
		   &= \underset{k\in\mathcal{G}}{\mathrm{argmax}} \frac{N_k}{N}x^T\hat{\Sigma}_T^{-1}\hat{\mu}_k + \frac{N_k}{N}
\end{align*}
$$

$$
\delta_k(x) = \frac{N_k}{2N}\{ (x-\hat{\mu}_k)^T\hat{\Sigma}_T^{-1}(x-\hat{\mu}_k)- x^T\hat{\Sigma}_T^{-1}x - \hat{\mu}_k^T\Sigma_T^{-1}\hat{\mu}_k-2 \}
$$

$$
b_k^{LDA} = \hat{\Sigma}_W^{-1}\hat{\mu}_k, \ b_k^{LR} = \hat{\Sigma}_T^{-1}\hat{\mu}_k \ \text{for} \ k=1,,,.K
$$

$$
\mathbf{B}^{LDA} = (b_1^{LDA}, ..., b_K^{LDA})
$$








$$
\hat{G}(x)  = \underset{k\in\mathcal{G}}{\mathrm{argmin}} \frac{1}{N} \| y_k - \mathbf{X}b_k\|^2 \ \text{where} \ b_k = b_k^{LR}
$$


$$
\hat{G}(x) = \underset{k\in\mathcal{G}}{\mathrm{argmin}} \frac{1}{N} \| y_k - \mathbf{X}b_k\|^2 - b_k^T\hat{\Sigma}_Bb_k \ \text{where} \ b_k = b_k^{LDA}
$$

$$
\delta_k(x) = -\frac{1}{2} \log |\Sigma_k| - \frac{1}{2} (x-\mu_k)^T(x-\mu_k) + \log \pi_k
$$


$$
\delta_k(x) = (x-\hat{\mu}_k)^T\hat{\Sigma}_W^{-1}(x-\hat{\mu}_k)
$$

$$
\delta_k(x) = -\frac{1}{2}(x-\hat{\mu}_k)^T\hat{\Sigma}_W^{-1}(x-\hat{\mu}_k)  + \log{\frac{N_k}{N}}
$$

$$
\delta_k(x) =  (x-\hat{\mu}_k)^T\hat{\Sigma}_T^{-1}(x-\hat{\mu}_k)- \hat{\mu}_k^T\Sigma_T^{-1}\hat{\mu}_k
$$

$$
\underset{a}{\mathrm{max}} \frac{a^T\mathbf{B}a}{a^T\mathbf{W}a}
$$

$$
\mathbf{W} = \mathbf{V}_W \mathbf{D}_W \mathbf{V}_W^T = (\mathbf{D}_W^{\frac{1}{2}} \mathbf{V}_W^T)^T(\mathbf{D}_W^{\frac{1}{2}} \mathbf{V}_W^T) = {\mathbf{W}^{\frac{1}{2}}}^T\mathbf{W}^{\frac{1}{2}}
$$

$$
\underset{a}{\mathrm{max}} \frac{a^T\mathbf{B}a}{a^T\mathbf{W}a} = \underset{b}{\mathrm{max}} \frac{b^T{\mathbf{W}^{-\frac{1}{2}}}^T\mathbf{B}\mathbf{W}^{-\frac{1}{2}}b}{b^Tb} = \underset{b}{\mathrm{max}} \frac{b^T\mathbf{B}^*b}{b^Tb}
$$

$$
\underset{b}{\mathrm{max}} b^T\mathbf{B}^*b - \lambda(b^Tb - 1), \ \mathbf{B}^*b = \lambda b
$$

$$
\log{\frac{Pr(G=k|X=x)}{Pr(G=K|X=x)}} = \beta_{k0} + \beta_k^Tx \ \text{for} \ k = 1,...,K-1
$$

$$
\theta = \{ \beta_{10}, \beta_1^T,...,\beta_{(K-1)0}, \beta_{K-1}^T, \ Pr(G=k|X=x) = p_k(x;\theta)
$$

$$
Pr(G=k|X=x) = \frac{\exp(\beta_{k0}+\beta_k^Tx)}{1+\sum_{l=1}^{K-1}\exp(\beta_{l0}+\beta_l^Tx)} \ \text{for} \ k = 1,...,K-1
$$

$$
Pr(G=K|X=x) = \frac{1}{1+\sum_{l=1}^{K-1}\exp(\beta_{l0}+\beta_l^Tx)}
$$

$$
\begin{align*}
l(\beta) &= \sum_{i=1}^N \{ y_i \log{p(x_i;\beta)} + (1-y_i) \log{(1-p(x_i;\beta))}\} \\
		 &= \sum_{i=1}^N \{ y_i\beta^Tx_i - \log{(1+\exp(\beta^Tx_i))} \}
\end{align*}
$$

$$
\frac{\partial l(\beta)}{\partial \beta} = \sum_{i=1}^N x_i(y_i - p(x_i;\beta)) = \mathbf{X}^T(\mathbf{y} - \mathbf{p}) \ \text{where} \ \mathbf{p} \ \text{the vector of} \ p(x_i;\beta^{old})
$$

$$
\frac{\partial^2 l(\beta)}{\partial \beta \partial \beta^T} = -\sum_{i=1}^Nx_ix_i^Tp(x_i;\beta)(1-p(x_i;\beta)) = -\mathbf{X}^T\mathbf{W}\mathbf{X} \ \text{where} \  \mathbf{W} \text{ a diagonal matrix with} \\ \mathbf{W}_{ii} = p(x_i;\beta^{old})(1-p(x_i;\beta^{old})).
$$

$$
\begin{align*}
\beta^{new} &= \beta^{old} + (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T(\mathbf{y}-\mathbf{p}) \\
			&= (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}(\mathbf{X}\beta^{old} +\mathbf{W}^{-1}(\mathbf{y} -\mathbf{p})) \\
			&= (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}\mathbf{z}, \ \mathbf{z}:\text{adjusted response}
\end{align*}
$$

$$
\beta^{new} \leftarrow \underset{\beta}{\mathrm{argmin}} (\mathbf{z}-\mathbf{X}\beta)^T\mathbf{W}(\mathbf{z}-\mathbf{X}\beta)
$$

$$
l(\beta) = \sum_{i=1}^N \sum_{k=1}^{K-1}[I(y_i = k)\beta_k^Tx_i - \log(1+\sum_{l=1}^{K-1}\exp(\beta_l^Tx_i))]
$$

$$
\mathbf{z} = \mathbf{X}\hat{\beta} +\mathbf{W}^{-1}(\mathbf{y} -\hat{\mathbf{p}})
$$

$$
\hat{\beta} = (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}\mathbf{z}
$$

$$
z_i = x_i^T\hat{\beta} + \frac{(y_i - \hat{p}_i)}{\hat{p}_i(1-\hat{p}_i)}, \ w_i = \hat{p_i}(1-\hat{p}_i)
$$

$$
D = -2l(\beta) + 2l(\text{perfect fitting})
$$

$$
\begin{align*}
D &= -2\sum_{i=1}^N \{ y_i \log{\hat{p}_i} + (1-y_i) \log{(1-\hat{p}_i)}\} + 2\sum_{i=1}^N \{ y_i \log{y_i} + (1-y_i) \log{(1-y_i)}\} \\
		 &= 2\sum_{i=1}^N [y_i \log{\frac{y_i}{\hat{p}_i}} + (1-y_i) \log{\frac{1-y_i}{1-\hat{p}_i}}]\\
		 &\approx 2\sum_{i=1}^N[ (y_i-\hat{p}_i) + \frac{(y_i-\hat{p}_i)^2}{2\hat{p}_i} + \{(1-y_i)-(1-\hat{p}_i)\} + \frac{\{(1-y_i)-(1-\hat{p}_i)\}^2}{2(1-\hat{p}_i)}] \\
		 &= \sum_{i=1}^N\frac{(y_i-\hat{p}_i)^2}{\hat{p_i}} + \frac{y_i-\hat{p}_i}{1-\hat{p}_i} \\
		 &= \sum_{i=1}^N\frac{(y_i-\hat{p}_i)^2}{\hat{p_i}(1-\hat{p_i})}
\end{align*}
$$

$$
\begin{align*}
\mathbb{E}[\hat{\beta}] &= \mathbb{E}[(\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}\mathbf{z}] \\
						&= (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}\mathbb{E}[\mathbf{X}\beta +\mathbf{W}^{-1}(\mathbf{y} -\mathbf{p})] \\
						&= (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}\mathbf{X}\beta =\beta
\end{align*}
$$

$$
y_i \overset{\mathrm{iid}}{\sim} Bernoulli(p_i)
$$

$$
\mathbb{E}[\mathbf{y}] = \mathbf{p}, \ Var[\mathbf{y}] = \mathbf{W}
$$

$$
\begin{align*}
Var[\hat{\beta}] &= Var[(\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}\mathbf{z}] \\
						&= (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}Var[\mathbf{X}\beta +\mathbf{W}^{-1}(\mathbf{y} -\mathbf{p})]((\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W})^T \\
						&= (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W}(\mathbf{W}^{-1}\mathbf{W}{\mathbf{W}^{-1}}^T)((\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}\mathbf{X}^T\mathbf{W})^T \\
						&= (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1}
\end{align*}
$$

$$
\hat{\beta} \sim \mathcal{N}(\beta, (\mathbf{X}^T\mathbf{W}\mathbf{X})^{-1})
$$

$$
\underset{\beta_0,\beta}{\mathrm{max}} \left[ \sum_{i=1}^N\{y_i(\beta_0 + \beta^Tx_i) - \log(1+\exp(\beta_0 + \beta^Tx))\} - \lambda\sum_{j=1}^p|\beta_p| 	\right]
$$

$$
\frac{\partial l}{\partial \beta_j} = \mathbf{x}_j^T(\mathbf{y} - \mathbf{p}) - \lambda \cdot \text{sign}(\beta_j) = 0, \ \mathbf{x}_j^T(\mathbf{y} - \mathbf{p}) = \lambda \cdot \text{sign}(\beta_j)
$$

$$
\underset{\beta_0,\beta}{\mathrm{min}} \ D(\beta, \beta_0) = \underset{\beta_0,\beta}{\mathrm{min}}-\sum_{i \in \mathcal{M}} y_i(x_i^T\beta + \beta_0)
$$

$$
\frac{1}{\|\beta\|}y_i(x_i^T\beta + \beta) \geq M, \ \text{and set} \ \|\beta\| = \frac{1}{M}
$$

$$
\beta = \sum_{i=1}^N\alpha_iy_ix_i, \ \sum_{i=1}^N\alpha_iy_i = 0
$$

$$
L_D = \sum_{i=1}^N \alpha_i - \frac{1}{2}\sum_{i=1}^N\sum_{k=1}^N\alpha_i\alpha_ky_iy_kx_i^Tx_k  \ \text{subject to} \ \alpha_i \geq 0
$$

$$
1 - y_i(x_i^T\beta + \beta) \leq 0 \ \text{for all} \ i
$$

$$
\alpha_i [y_i(x_i^T\beta + \beta_0)-1] = 0 \ \text{for all} \ i.
$$

$$
\beta = \sum_{i\in\mathcal{S}}\alpha_iy_ix_i
$$



